# RAG Chatbot Documentation

Welcome to the RAG Chatbot documentation! This folder contains all guides and documentation for the project.

## ðŸ“š Documentation Index

### Getting Started
1. **[Llama Setup Guide](./LLAMA_SETUP.md)** - Complete guide to setting up Ollama and Llama models
2. **[Usage Guide](./USAGE.md)** - How to use ingestion and chat scripts
3. **[GPU Setup Guide](./GPU_SETUP.md)** - GPU acceleration for faster performance
4. **[Project README](./README.md)** - Original project overview

### Quick Links
- **Installation**: See [LLAMA_SETUP.md](./LLAMA_SETUP.md#installation)
- **First Time Setup**: See [USAGE.md](./USAGE.md#setup)
- **Troubleshooting**: See [LLAMA_SETUP.md](./LLAMA_SETUP.md#troubleshooting)

## ðŸš€ Quick Start

### 1. Install Ollama
```bash
# Download from: https://ollama.com/download
ollama pull llama3
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the Chatbot
```bash
streamlit run rag_chat.py
```

## ðŸŽ¯ Features

- âœ… **Local Llama Models** - Free, private, and fast
- âœ… **GPU Acceleration** - Automatic GPU detection and usage
- âœ… **Multiple LLM Providers** - Llama, OpenAI, Google
- âœ… **Flexible Embeddings** - Llama, HuggingFace, OpenAI, Google
- âœ… **PDF Support** - Process healthcare documents
- âœ… **Conversation Memory** - Context-aware responses
- âœ… **Source Citations** - See where answers come from

## ðŸ’¡ Best Practices

### For Speed:
- Use HuggingFace embeddings with GPU
- Use smaller Llama models: `phi3` or `mistral`

### For Quality:
- Use `llama3.1` models
- Increase `top_k` documents

### For Privacy:
- Use Llama models (everything runs locally)

## ðŸ“– Detailed Guides

- **[LLAMA_SETUP.md](./LLAMA_SETUP.md)** - Ollama and Llama models setup
- **[GPU_SETUP.md](./GPU_SETUP.md)** - GPU acceleration setup
- **[USAGE.md](./USAGE.md)** - Complete usage guide with examples
